{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmDfMAHwIfliWvDYYAEJcy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TerryTian21/PyTorch-Practice/blob/main/PyTorch_Video_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Going Modular"
      ],
      "metadata": {
        "id": "Kb6ofR3KMRw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## What is going modular?\n",
        "\n",
        " Going modular involves turning notebook code (Jupyter Notebooks) into a series of different Python scripts that offer similar functionality.\n",
        "\n",
        " For example, we could turn our notebook code from a series of cells into pythone files\n",
        "\n",
        " * `data_setup.py` - Prepare and download data if needed\n",
        " * `engine.py` - Training functions\n",
        " * `model_builder.py` - Creates desired Pytorch models\n",
        "\n",
        "Note: Naming of files depends on use case and code requirements."
      ],
      "metadata": {
        "id": "uVavscENMXBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why go Modular\n",
        "\n",
        "While notebooks are great for exploration purposes, larger scale porjects may find Python scripts more reproducible and easier to run.\n",
        "\n",
        "*Production Code* is code that runs to offer a service to someone. Libraries like `nb-dev` allow you to write Python libraries in Jupyer Notebooks.\n",
        "\n",
        "Usually, machine learning projects start in notebooks and the useful pieces are moved into Python scripts once they are confirmed to be workign as intended."
      ],
      "metadata": {
        "id": "K9f0Xd9ATPNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Things to Note\n",
        "\n",
        "**Docstrings** - Writing reproducible and understandable code is important and functions should be created with docstrings.\n",
        "\n",
        "**Imports at the top** - All scripts require import modules be imported at the start of the script"
      ],
      "metadata": {
        "id": "353xryGVTpub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Moving Cells to Scripts"
      ],
      "metadata": {
        "id": "S9nsYVZjTuiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Data\n",
        "\n",
        "Here we create a script for obtaining the image data for our model"
      ],
      "metadata": {
        "id": "KmBCvwKHU4Nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# If the image folder doesn't exist, and and prepare\n",
        "if image_path.is_dir():\n",
        "  print(f\"Path Exists\")\n",
        "else:\n",
        "  print(f\"Creating Image Folder\")\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download Dataset\n",
        "with open(data_path / \"images.zip\", \"wb\") as f:\n",
        "  response = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "  f.write(response.content)\n",
        "\n",
        "# Unzip images\n",
        "with zipfile.ZipFile(data_path / \"images.zip\", \"r\") as zip_ref:\n",
        "  zip_ref.extractall(data_path)\n",
        "\n",
        "# Remove zip file\n",
        "os.remove(data_path / \"images.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNXh0EoHVAC2",
        "outputId": "65ac43e6-433d-4ecc-b3ed-9765dc84a3ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Image Folder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"going_modular\", exist_ok=True)"
      ],
      "metadata": {
        "id": "joYVNp0AYT-x"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Dataset and DataLoader\n",
        "\n",
        "We can convert the dataset and dataloader creation code into a function called create_dataloaers()\n",
        "\n",
        "We can write it to a file by using the line `%%writefile`"
      ],
      "metadata": {
        "id": "jcO2LTtRWdb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/data_setup.py\n",
        "\"\"\"\n",
        "Contains functions for create Python Dataloaders for image classification\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaers(train_dir: str, test_dir:str, transfrom: transforms.Compose, batch_size: int, num_workers: int=NUM_WORKERS):\n",
        "  \"\"\" Creates training and testing DataLoaders\n",
        "\n",
        "  takes in training/testing directory path and turns them into Datasets and DataLoaders\n",
        "\n",
        "  Args:\n",
        "    train_dir (str): Path to training directory\n",
        "    test_dir (str): Path to testing directory\n",
        "    transfrom (transforms.Compose): transform to perform on training and test data\n",
        "    batch_size (int): number of samples per batch in datalaoder\n",
        "    num_workers (int, optional): Number of workers for dataloader. Defaults to NUM_WORKERS\n",
        "\n",
        "  Returns:\n",
        "    A tuple of (train_dataloader, test_dataloader, class_names) where class_names is a list of possible labels\n",
        "    Example usage:\n",
        "      train_dataloader, test_dataloader, class_names = create_dataloaers(train_dir=path/to/train, test_dir=path/to/test, transfrom, batch_size=32, num_workers=4)\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  #Use Image folder to create datasets\n",
        "\n",
        "  train_dataset = datasets.ImageFolder(train_dir, transform=transfrom)\n",
        "  test_dataset = datasets.ImageFolder(test_dir, transform=transfrom)\n",
        "\n",
        "  #Turn image into dataloaders\n",
        "\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "  #Get class names\n",
        "  class_names = train_dataset.classes\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jbEUfJDWuRu",
        "outputId": "4542ecce-e8e2-498f-965a-fd64f3919e2a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to create dataloader's we can just import the function from `data_setup.py`"
      ],
      "metadata": {
        "id": "iYjC4YxcZtpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import function\n",
        "from going_modular import data_setup\n"
      ],
      "metadata": {
        "id": "1Ou82zOGZ1re"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making a Model\n",
        "\n",
        "We have repeatedly used the TinyVGG model in the past and so to avoid repetition we can create a script for it"
      ],
      "metadata": {
        "id": "NqgPRlgfaFxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/model_builder.py\n",
        "\"\"\"\n",
        "Creates an instance of the ResNet34 architecture\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class ResNet34(nn.Module):\n",
        "  \"\"\"Creates the ResNet34 architecture\n",
        "\n",
        "  Replicates the ResNet34 Architecture from PyTorch\n",
        "\n",
        "  Args:\n",
        "    input_shape: An integer representing the number of channels in the input image\n",
        "    hidden_units: An integer representing the number of nodes inbetween layers\n",
        "    output_shape: An integer representing the number of classes\n",
        "    layers: A list of the number of blocks in each layer\n",
        "\n",
        "  Returns:\n",
        "    A PyTorch model\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  class BasicBlock(nn.Module):\n",
        "\n",
        "    \"\"\" A basic building block of the ResNet34 architecture\n",
        "\n",
        "    Consists of two convolutional layers with batch normalization and ReLU activations.\n",
        "    This is a residual block where the input to the convolutions is added to the output of the convolutions\n",
        "\n",
        "    Args:\n",
        "      in_features: The input shape of the data\n",
        "      out_features: The output shape of the data\n",
        "      stride: The stride of the convolutions\n",
        "      downsample: An optional downsampling layer for when the stride is not 1\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, stride=1, downsample=None):\n",
        "      super().__init__()\n",
        "      self.conv = nn.Conv2d(in_features, out_features, kernel_size=3, stride=stride, padding=1)\n",
        "      self.bn = nn.BatchNorm2d(out_features)\n",
        "      self.relu = nn.ReLU(inplace=True)\n",
        "      self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      identity = x\n",
        "\n",
        "      out = self.conv(x)\n",
        "      out = self.bn(x)\n",
        "      out = self.relu(out)\n",
        "      out = self.conv(out)\n",
        "      out = self.bn(out)\n",
        "\n",
        "      if self.downsample is not None:\n",
        "        identity = self.downsample(x)\n",
        "\n",
        "      out += identity\n",
        "      out = self.relu(out)\n",
        "\n",
        "      return out\n",
        "\n",
        "  # Init function of the Resnet Model\n",
        "  def __init__(self, input_shape, hidden_units, output_shape, layers):\n",
        "    super().__init__()\n",
        "    self.hidden_units = hidden_units\n",
        "    self.layers=layers\n",
        "\n",
        "    self.conv_block = nn.Sequential(\n",
        "        nn.Conv2d(input_shape, hidden_units, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "        nn.BatchNorm2d(num_features=hidden_units),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "    )\n",
        "\n",
        "    self.layer1 = self.make_layers(hidden_units, layers[0], stride=1)\n",
        "    self.layer2 = self.make_layers(hidden_units*2, layers[1], stride=2)\n",
        "    self.layer3 = self.make_layers(hidden_units*4, layers[2], stride=2)\n",
        "    self.layer4 = self.make_layers(hidden_units*8, layers[3], stride=2)\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.classifier=nn.Linear(512, output_shape)\n",
        "\n",
        "  # Sets the number of blocks in a layer\n",
        "  def make_layers(self, out_features, blocks, stride):\n",
        "    downsample = None\n",
        "\n",
        "    if stride != 1 or self.hidden_units != out_features:\n",
        "      downsample = nn.Sequential(\n",
        "          nn.Conv2d(self.hidden_units, out_features, kernel_size=1, stride=2, bias=False),\n",
        "          nn.BatchNorm2d(out_features)\n",
        "      )\n",
        "\n",
        "    layers = []\n",
        "\n",
        "    # The first block with custom stride\n",
        "    layers.append(self.BasicBlock(self.hidden_units, out_features, stride, downsample))\n",
        "    self.hidden_units = out_features\n",
        "\n",
        "    # Each block after has default stride value\n",
        "    for _ in range(1, blocks):\n",
        "      layers.append(self.BasicBlock(self.hidden_units, out_features))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_block(x)\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.layer4(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    return self.classifier(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gO5YKxU0aPCV",
        "outputId": "00f8d929-71fe-452c-8033-1bed6e27813c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/model_builder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from going_modular import model_builder\n",
        "\n",
        "class_names = [\"pizza\", \"steak\", \"sushi\"]\n",
        "\n",
        "torch.manual_seed(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Instantiate Model\n",
        "layers = [3, 4, 6, 3]\n",
        "model = model_builder.ResNet34(input_shape=3, hidden_units=64, layers=layers, output_shape=len(class_names))"
      ],
      "metadata": {
        "id": "y6Ze6C9ql_nW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "id": "berdViUxpkYu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "557025b6-5349-4fd7-ec23-cd62dfb5a4e2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet34(\n",
              "  (conv_block): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): BasicBlock(\n",
              "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (classifier): Linear(in_features=512, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Train and Test Functions\n",
        "\n",
        "We previously functionized the training loop into three different functions:\n",
        "\n",
        "* `train_step()`\n",
        "* `test_step()`\n",
        "* `train()`\n",
        "\n",
        "Since these will be the engine of our model we can put them into a Python script called `engine.py`"
      ],
      "metadata": {
        "id": "Wn2fbL6orH8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/engine.py\n",
        "\"\"\"\n",
        "Contains functions for training and testing a PyTorch model.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from torch import nn\n",
        "\n",
        "def train_step(mode, dataloader, loss_fn, optimizer, device):\n",
        "  \"\"\"\n",
        "  Train's a Pytorch model for a single epoch.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained.\n",
        "    dataloader: A PyTorch dataloader for training data\n",
        "    loss_fn: The loss function\n",
        "    optimizer: The optimizer used to minimized the loss function\n",
        "    device: The target device used for computations\n",
        "\n",
        "  Returns:\n",
        "    A tuple of training loss and training accuracy metrics\n",
        "  \"\"\"\n",
        "\n",
        "  #Put model in trian mode\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  train_loss, train_acc = 0, 0\n",
        "  for (X,y) in dataloader:\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    y_logits = model(X)\n",
        "    loss = loss_fn(y_logits, y)\n",
        "    y_preds = y_logits.softmax(dim=1).argmax(dim=1)\n",
        "\n",
        "\n",
        "    train_loss += loss\n",
        "    train_acc += ((y_preds == y).sum().item() / len(y_preds))\n",
        "\n",
        "    # Optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss, train_acc = train_loss / len(dataloader), train_acc / len(dataloader)\n",
        "  return train_loss, train_acc\n",
        "\n",
        "def test_step(model, dataloader, loss_fn, device):\n",
        "  \"\"\"\n",
        "  Tests a PyTorch model for one epoch\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained.\n",
        "    dataloader: A PyTorch dataloader for testing data\n",
        "    loss_fn: The loss function\n",
        "    device: The target device used for computations\n",
        "\n",
        "  Returns:\n",
        "    A tuple of test loss and test accuracy metrics\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "\n",
        "  test_loss, test_acc = 0, 0\n",
        "  with torch.infernce_mode():\n",
        "    for (X,y) in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      y_logits = model(X)\n",
        "      test_loss += loss_fn(y_logits, y)\n",
        "\n",
        "      y_preds = y_logits.softmax(dim=1).argmax(dim=1)\n",
        "      test_acc += ((y_preds == y_logits).sum().item() / len(y_preds))\n",
        "\n",
        "    test_loss, test_acc = test_loss / len(dataloader), test_acc / len(dataloader)\n",
        "\n",
        "  return test_loss, test_acc\n",
        "\n",
        "\n",
        "def train(model, train_dataloader, test_dataloader, loss_fn, optimizer, epochs, device):\n",
        "  \"\"\"\n",
        "  Trains and Tests A Pytorch Model\n",
        "\n",
        "  Passes a target model through train_step() and test_step() for a number of epochs.\n",
        "\n",
        "  Returns:\n",
        "    A dictionary of training and testing loss and acc metrics.\n",
        "\n",
        "  \"\"\"\n",
        "  print(f\" Starting Training ----------------------\")\n",
        "  results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
        "\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "\n",
        "    train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
        "    test_loss, test_acc = test_step(model, test_dataloader, loss_fn, device)\n",
        "\n",
        "    results[\"train_loss\"] += train_loss\n",
        "    results[\"train_acc\"] += train_acc\n",
        "    results[\"test_loss\"] += test_loss\n",
        "    results[\"test_acc\"] += test_acc\n",
        "\n",
        "    print(f\"Epoch: {epoch} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f} | Test Loss: {test_loss:.3f} | Test Acc: {test_acc:.2f}\")\n",
        "\n",
        "\n",
        "  return results\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7XjulOVqB_x",
        "outputId": "d569c6a9-ce1f-4369-a944-0c1342c2c08a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Util Functions\n",
        "\n",
        "Util functions that come in handy such as saving and loading a model"
      ],
      "metadata": {
        "id": "yNkO7lnt_ENb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/utils.py\n",
        "\"\"\"\n",
        "Contains various utility functions\n",
        " \"\"\"\n",
        "\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def save_model(model, target_dir, model_name):\n",
        "  \"\"\"\n",
        "  Saves a PyTorch model to a target directory\n",
        "\n",
        "  Args:\n",
        "    model: A target PyTorch model to save\n",
        "    target_dir: Path the the directory\n",
        "    modle_name: name for model\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Create target directory\n",
        "\n",
        "  target_dir_path = Path(target_dir)\n",
        "  target_dir_path.mkdir(parents=True, exists_ok=True)\n",
        "\n",
        "  # Create Model Save Path\n",
        "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name shoud end in '.pt' or '.pth'\"\n",
        "  model_save_path = target_dir_path / model_name\n",
        "\n",
        "  # Save Model\n",
        "  print(\"Saving Model\")\n",
        "  torch.save(obj=model.state_dict(), f=model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhN3Mpp3_M8f",
        "outputId": "d30a9469-600d-45c4-8354-36d4b392ee18"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting it Together\n",
        "\n",
        "Now that we have all of our Python files we can combine them to create a single script `train.py`\n",
        "\n",
        "This way, we can train our PyTorch model with a single line of code. The steps for doing so are\n",
        "\n",
        "1. Import the necessary dependcy files (including the previous modules)\n",
        "2. Set up hyperparameters\n",
        "3. Set up train and test directories\n",
        "4. Create necessary data transforms\n",
        "5. Combine modules to train the model"
      ],
      "metadata": {
        "id": "4-uf_nAlA24v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/train.py\n",
        "\"\"\"\n",
        "Trains a Pytorch image classification model and stores in models directory\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import data_setup, engine, model_builder, utils\n",
        "\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "# Setup Hyperparameters\n",
        "\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 64\n",
        "HIDDEN_UNITS=64\n",
        "LEARNING_RATE = 0.001\n",
        "LAYERS= [3,4,6,3]\n",
        "\n",
        "# Directories\n",
        "train_dir = \"data/pizza_steak_sushi/train\"\n",
        "test_dir = \"data/pizza_steak_sushi/test\"\n",
        "\n",
        "# Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Transforms\n",
        "transform = v2.Compose([v2.PILToTensor(),\n",
        "                        v2.Resize((224,224), antialias=True)])\n",
        "\n",
        "# Get the dataloaders\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaers(train_dir, test_dir, transform, BATCH_SIZE)\n",
        "\n",
        "# Create mdoel\n",
        "model = model_builder.ResNet34(input_shape=3, hidden_units=HIDDEN_UNITS, output_shape=len(class_names), layers=LAYERS).to(device)\n",
        "\n",
        "# Create loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(),\n",
        "                             lr=LEARNING_RATE)\n",
        "#Train\n",
        "engine.train(model, train_dataloader, test_dataloader, loss_fn, optimizer, EPOCHS, device)\n",
        "\n",
        "# Save the model\n",
        "utils.save_model(model, \"models\", \"ResNet34.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l57TWJXeCQ_E",
        "outputId": "cdf1ed08-0715-43f9-e675-777732b6f9ae"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/train.py\n"
          ]
        }
      ]
    }
  ]
}